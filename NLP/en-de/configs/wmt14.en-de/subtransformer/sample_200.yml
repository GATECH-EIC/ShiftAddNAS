encoder-embed-dim-subtransformer: 640
decoder-embed-dim-subtransformer: 640

encoder-ffn-embed-dim-all-subtransformer: [2048, 3072, 2048, 3072, 1024, 2048]
decoder-ffn-embed-dim-all-subtransformer: [1024, 1024, 3072]

encoder-layer-num-subtransformer: 6
decoder-layer-num-subtransformer: 3

encoder-self-attention-heads-all-subtransformer: [4, 4, 4, 4, 8, 4]
decoder-self-attention-heads-all-subtransformer: [4, 8, 4]
decoder-ende-attention-heads-all-subtransformer: [4, 4, 4]

# for arbitrary encoder decoder attention. -1 means attending to last one encoder layer
# 1 means last two encoder layers, 2 means last three encoder layers
decoder-arbitrary-ende-attn-all-subtransformer: [-1, 1, 2]

encoder-block-types-all-subtransformer: [self_attention, self_attention, lightweight_conv, lightweight_shiftadd, self_attention, self_attention]
decoder-block-types-all-subtransformer: [lightweight_conv, lightweight_add, self_attention]

 # config for lowest  loss (2.489174424910365) model:
 # {encoder:
 #  {encoder_embed_dim: 640,
 #  encoder_layer_num: 6,
 #  encoder_ffn_embed_dim: [2048, 3072, 2048, 3072, 1024, 2048],
 #  encoder_self_attention_heads: [4, 4, 4, 4, 8, 4],
 #  encoder_block_types: [self_attention+lightweight_shiftadd, self_attention+lightweight_shiftadd, lightweight_conv, lightweight_shiftadd, self_attention+lightweight_shiftadd, lightweight_shiftadd]},
 #  decoder: {decoder_embed_dim: 640,
 #  decoder_layer_num: 3,
 #  decoder_ffn_embed_dim: [1024, 1024, 3072, 1024, 1024, 1024],
 #  decoder_self_attention_heads: [4, 8, 4, 8, 8, 4],
 #  decoder_ende_attention_heads: [4, 4, 4, 4, 8, 8],
 #  decoder_arbitrary_ende_attn: [-1, 1, 2, 2, -1, -1],
 #  decoder_block_types: [lightweight_conv, lightweight_shiftadd, lightweight_shiftadd, self_attention+lightweight_shiftadd, self_attention+lightweight_add, lightweight_add]}}
